# Data-Efficient Hierarchical Reinforcement Learning
Implementation practice of RL project, the second phase

### Reading Contents:

* [Intro to HIRO](# Intro to HIRO)
* Understand value-based RL method: [From Q-Learning to TD3](./From Q-Learning to TD3.md)
* Code structure of this HIRO implementation
* Start to use this code base
* Experiment results

## Intro to HIRO

Original Paper: https://arxiv.org/abs/1805.08296

### General Intro

HIRO represents "HIerarchical Reinforcement learning with Off-policy correction". The motivation of this paper is to train both HRL low-level policy and high-level policy with off-policy experience. The author think this will help improve data efficiency and makes the method more generally applicable to real-world tasks.

HIRO can be roughly introduced with three paragraphs (paper 3.1-3.3):

* Hierarchical Frameworks: Low-level policy $\mu^{lo}$ generates action $a$ to reach a goal $g$, it directly interact with environment and get reward $r$. High-level policy $\mu^{hi}$ takes the low-level behavior as observation, then generates goals intermittently (every $c$ steps) for low-level policy to pursue. low-level policy and high-level policy are both trained with off-policy algorithm TD3, which follows standard RL setting.
* Parameterized Reward: $\mu^{lo}$ takes $(state, goal)$ as its observation to generate action at each time step. so the off-policy experience tuple for low-level policy is $(s_t, g_t, a_t, r_t, s_{t+1}, g_{t+1})$. Since the task for low-level policy is to reach the goal state generated by high-level policy, i.e., let $s_{t+c} = s_t + g_t$ happen, it takes a dense parameterized reward that represents the L2 distance of current state and goal state, i.e., $r(s_t, a_t, g_t, s_{t+1}) = -||s_t, a_t, g_t, s_{t+1}||_2$. On the other hand, the high-level policy takes the returned provided by environment to encourage it to generate good goal sequences that leads the agent to achieve more rewards. The off-policy experience tuple for low-level policy is $(s_t, g_t, \sum{r_{t:t+c-1}}, s_{t+c})$. 
* Off-Policy Correction: Transitions obtained from past lower-level controllers do not accurately reflect the actions (and therefore resultant states $s_{t+1:t+c}$) that would occur if the same goal were used with the current lower-level controller. HIRO introduces a correction that translates old transitions into ones that agree with the current lower-level controller. The method is to select the goal that most likely to let current low-level policy to induce the same low-level behavior with the current instantiation of the lower-level policy. This surrogate goal $\hat{g}$ is generated form a set of candidate goals then replace the original goal in high-level off-policy experience tuple. The paper claims this method works well empirically then also discussed about other methods of applying this off-policy correction.

To conclude, form my point of view, HIRO paper majorly contributes to:

* provided an HRL framework that lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers.
* improve traditional HRL methods at data-efficiency and real-world task generalization ability by utilizing off-policy training algorithm
* proposed off-policy correction method to leverage the low-level/high-level policy incompetence problem introduced by the off-policy algorithm and proves the correction method works well.

### Algorithm

#### **Method Review**

HIRO selects TD3 (paper 2.1) as its off-policy training algorithm. The algorithm for HIRO is basically a combination of TD3 and hierarchical RL framework. 

The low-level and high-level policy is trained with TD3 as usual, but with modified rewards: Low-level policy uses a dense intrinsic reward provided by high-level policy, which is the L2 distance of current state to goal state. High-level policy uses the cumulated reward from environment, which is provided by low-level policy. (paper 3.1-3.2)

To apply off-policy correction, HIRO selects an approximately optimal $\hat{g}$ as a surrogate of $g$ to train high-level policy. Current method is to select the $\hat{g}$ that provides the biggest likelihood to generate past induced low-level behavior. (paper 3.3, C.3, A)

The original paper does not provide algorithm graph, we can write our own here:

#### **Algorithm HIRO**

​		Initialize critic networks $Q_{\theta_1^{low}}$, $Q_{\theta_2^{low}}$, $Q_{\theta_2^{high}}$, $Q_{\theta_2^{high}}$ and actor networks $\mu_{\phi_1^{low}}$, $\mu_{\phi_2^{low}}$, $\mu_{\phi_1^{high}}$, $\mu_{\phi_1^{high}}$ with random $\theta s$ and $\phi s$ 

​		Initialize target networks $\theta_1^{'low} \leftarrow \theta_1^{low}$,  $\theta_2^{'low} \leftarrow \theta_2^{low}$,  $\theta_1^{'high} \leftarrow \theta_1^{high}$,  $\theta_2^{'high} \leftarrow \theta_2{high}$, $\phi_1^{'low} \leftarrow \phi_1^{low}$, $\ldots$

​		Initialize replay buffer $\Beta^{low}$, $\Beta^{high}$

​		**for** $t = 1$ **to** $T$ **do**

​				select action with exploration noise $a_t \sim \mu(s_t, g_t) + \epsilon, \epsilon \sim \N(0, \sigma)$

​				observe reward $r$ and new state $s_{t+1}$, store transition tuple $(s, g, a, r, s', g')$ in $\Beta_{low}$

​				select goal with exploration noise  $g_t \sim \mu(s_{t-c:t}, g_{t-c:t}) + \epsilon, \epsilon \sim \N(0, \sigma)$

​				**if** $t$ mod $c$ **then**

​						apply off-policy correction $\hat{g} = correction(g)$, store transition tuple $(s_{t-c:t}, g_{t-c:t}, a_{t-c:t}, r_{t-c:t}, s_{t+c})$ in $\Beta_{high}$

​				**end if**

​				sample mini-batch of $N$ transactions $(s, g, a, r, s', g')$ from $\Beta_{low}$

​				$\hat{a} \leftarrow \mu_{\phi'}(s', g') + \epsilon, \epsilon \in \text{clip}(\N(0, \hat{\sigma}), -k, k)$

​				$y^{low} \leftarrow r + \gamma \text{min}_{i=1,2}Q_{\theta_i'}(s', g', \hat{a})$

​				update critics $\theta_i^{low} \leftarrow \text{argmin}_{\theta_i^{low}}N^{-1}\sum{(y^{low}-Q_{\theta_i^{low}}(s,a, g)})^2$ 

​				**if** $t$ mod $d$ **then**

​						update $\phi^{low}$ by the deterministic policy gradient:

​						soft update target network

​				**end if** 

​				**if** $t$ mod $c$ **then**

​						sample mini-batch of $N$ transactions $(s_{t-c:t}, g_{t-c:t}, a_{t-c:t}, r_{t-c:t}, s_{t+c})$ from $\Beta_{high}$

​						$\hat{g'} \leftarrow \mu_{\phi'}(s_{t-c:t}', g'_{t-c:t}) + \epsilon, \epsilon \in \text{clip}(\N(0, \hat{\sigma}), -k, k)$

​						$y^{high} \leftarrow r + \gamma \text{min}_{i=1,2}Q_{\theta_i'}(s'_{t-c:t}, g'_{t-c:t}, \hat{g'}_{t-c:t})$

​						update critics $\theta_i^{high} \leftarrow \text{argmin}_{\theta_i^{high}}N^{-1}\sum{(y^{high}-Q_{\theta_i^{high}}(s_{t-c:t}, g_{t-c:t}, a_{t-c:t})})^2$ 

​						**if** $t$ mod $d$ **then**

​								update $\phi^{high}$ by the deterministic policy gradient:

​								soft update target network

​						**end if** 

​				**end if**

​		**end for**

## Code Structure

The code structure of this implementation is much simpler compared with original implementation. This implementation basically follows the structure of a off-policy training code structure. This "Code Structure" segment is to provide a quick view of this implementation code.

*  train.py: training loop is the core code segment. It trains the algorithms and uses services provided by other codes.
* experience_buffer.py: holds the experience buffer class. 
* networks.py: holds NNs used in training process.
* utils.py: holds util classes, functions and frequently used data.

**Hint:**

* uses only positional parameter to form goal
* uses delayed parameter update interval to be 1

## How to Use This Repo

### requirements

Please make sure you have following software correctly installed before using this code base: 

* PyTorch
* OpenAI Gym (Mujoco)
* Weights & Biases

### Install

#### Clone Repo & Deploy

clone this code base:

> git clone https://github.com/ziangqin-stu/impl_data-effiient-hrl.git

go to */impl_data-effiient-hrl*, install required packages:

> pip install -e requirements.txt

#### Test Run

Set your wandb, modify *./test/td3.py* line 162 to your project name.

In terminal, go to project root path, type command to run a simple TD3 training process to test success of deploy:

```python
python run.py --algorithm=td3 --param_id=1
```

This experiment runs on CPU, the final results should be like this: 

<div style="display:flex;">
    <div style="display:flex; margin:auto;">
        <img src=".\readme_data\td3-idp-reward.png" alt="td3-idp-reward" width="600" style="padding: 5px;"/>
        <img src=".\readme_data\td3-idp-video.gif" alt="td3-idp-reward" width="300" style="padding: 5px;"/>
    </div>       
</div>

### Play Code

#### Command Line Interface

#### Evaluate Run

#### Training Run



## Experiment Results

### Ant Push

walk, straight, bypass, success

not a local minimum

### Ant Fall 

















































## Understanding: from vanilla HRL to HIRO

 ### HRL Basic Idea

#### Off-policy vs. On-policy

* Watch this video: [Reinforcement Learning Class: Off-policy vs On-policy](https://www.youtube.com/watch?v=hlhzvQnXdAA)

Basically, off-policy methods update action policy with rollouts generated by a different xx policy...

Previous HRL methods use on-policy algorithm, .... 

HIRO uses off-policy algorithms to improve ???

But using off-policy algorithm also introduces trouble: The high-level policy will generate a different target for lower-policy after it was (high-level policy) updated. HIRO puts forward a straight forward makeup to solve this problem.

### HIRO: XXXX

### Insights: YYYY

## Quickly Understand the Code: structures in this project

### Algorithm Structure

### Implementation Structure

## Try Yourself: how to run this code base from scratch

### Getting Started

### Command-Line Interface

### Checkpoint Feature

### Loggers

### Quick Commands to Run Experiments

## Experiments

### Intro to Launched Experiments

### Training Result

## Project Summary

...

